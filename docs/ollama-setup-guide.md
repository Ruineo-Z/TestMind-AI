# ğŸ¦™ Ollamaæœ¬åœ°AIéƒ¨ç½²æŒ‡å—

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©Ollamaï¼Ÿ

âœ… **å®Œå…¨å…è´¹** - æ— APIè´¹ç”¨  
âœ… **æ•°æ®å®‰å…¨** - æœ¬åœ°è¿è¡Œï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°  
âœ… **é«˜æ€§èƒ½** - é’ˆå¯¹æœ¬åœ°ç¡¬ä»¶ä¼˜åŒ–  
âœ… **å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒLlama2ã€Mistralã€CodeLlamaç­‰  
âœ… **æ˜“äºé›†æˆ** - ä¸LangChainå®Œç¾å…¼å®¹  

## ğŸš€ å¿«é€Ÿå®‰è£…

### Windowså®‰è£…
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
# è®¿é—® https://ollama.ai/download/windows
# æˆ–ä½¿ç”¨winget
winget install Ollama.Ollama
```

### macOSå®‰è£…
```bash
# ä½¿ç”¨Homebrew
brew install ollama

# æˆ–ä¸‹è½½å®‰è£…åŒ…
# https://ollama.ai/download/mac
```

### Linuxå®‰è£…
```bash
# ä¸€é”®å®‰è£…è„šæœ¬
curl -fsSL https://ollama.ai/install.sh | sh
```

## ğŸ“¦ æ¨èæ¨¡å‹ä¸‹è½½

### 1. Llama2 (æ¨èç”¨äºéœ€æ±‚åˆ†æ)
```bash
# 7Bæ¨¡å‹ (4GBå†…å­˜)
ollama pull llama2

# 13Bæ¨¡å‹ (8GBå†…å­˜) - æ›´å‡†ç¡®
ollama pull llama2:13b

# 70Bæ¨¡å‹ (40GBå†…å­˜) - æœ€å‡†ç¡®
ollama pull llama2:70b
```

### 2. Mistral (å¿«é€Ÿå“åº”)
```bash
# 7Bæ¨¡å‹ - é€Ÿåº¦å¿«
ollama pull mistral

# Mistral Instruct - æŒ‡ä»¤ä¼˜åŒ–
ollama pull mistral:instruct
```

### 3. CodeLlama (ä»£ç ç†è§£)
```bash
# ä»£ç ä¸“ç”¨æ¨¡å‹
ollama pull codellama

# Pythonä¸“ç”¨
ollama pull codellama:python
```

### 4. ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
```bash
# ä¸­æ–‡æ”¯æŒæ›´å¥½çš„æ¨¡å‹
ollama pull qwen:7b
ollama pull baichuan2:7b
```

## âš™ï¸ TestMind AIé…ç½®

### 1. ç¯å¢ƒå˜é‡é…ç½®
```bash
# .envæ–‡ä»¶
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2
AI_PROVIDER=ollama
```

### 2. ä»£ç ä½¿ç”¨ç¤ºä¾‹
```python
from app.requirements_parser.extractors.langchain_extractor import LangChainExtractor, AIProvider

# ä½¿ç”¨Ollamaæå–éœ€æ±‚
extractor = LangChainExtractor(
    provider=AIProvider.OLLAMA,
    model="llama2",  # æˆ– "mistral", "codellama"
    ollama_url="http://localhost:11434"
)

# æå–éœ€æ±‚
requirements = await extractor.extract_async(document)
```

### 3. æ€§èƒ½ä¼˜åŒ–é…ç½®
```python
# é’ˆå¯¹ä¸åŒç¡¬ä»¶çš„ä¼˜åŒ–é…ç½®
configs = {
    "ä½é…ç½®": {
        "model": "llama2:7b",
        "temperature": 0.1,
        "max_tokens": 1000
    },
    "ä¸­é…ç½®": {
        "model": "llama2:13b", 
        "temperature": 0.1,
        "max_tokens": 2000
    },
    "é«˜é…ç½®": {
        "model": "llama2:70b",
        "temperature": 0.05,
        "max_tokens": 4000
    }
}
```

## ğŸ”§ å¯åŠ¨å’Œç®¡ç†

### å¯åŠ¨OllamaæœåŠ¡
```bash
# å¯åŠ¨æœåŠ¡
ollama serve

# åå°è¿è¡Œ
nohup ollama serve > ollama.log 2>&1 &
```

### æ¨¡å‹ç®¡ç†
```bash
# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama list

# åˆ é™¤æ¨¡å‹
ollama rm llama2

# æ›´æ–°æ¨¡å‹
ollama pull llama2
```

### æ€§èƒ½ç›‘æ§
```bash
# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
ollama ps

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama2
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | é€Ÿåº¦ | å‡†ç¡®ç‡ | æ¨èç”¨é€” |
|------|------|----------|------|--------|----------|
| llama2:7b | 4GB | 8GB | â­â­â­â­â­ | â­â­â­ | å¿«é€ŸåŸå‹ |
| llama2:13b | 7GB | 16GB | â­â­â­â­ | â­â­â­â­ | ç”Ÿäº§ç¯å¢ƒ |
| mistral:7b | 4GB | 8GB | â­â­â­â­â­ | â­â­â­â­ | å¹³è¡¡é€‰æ‹© |
| codellama:7b | 4GB | 8GB | â­â­â­â­ | â­â­â­â­ | ä»£ç åˆ†æ |

## ğŸ¯ TestMind AIé›†æˆæµ‹è¯•

### 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
```python
# æµ‹è¯•Ollamaè¿æ¥
import requests

response = requests.post("http://localhost:11434/api/generate", json={
    "model": "llama2",
    "prompt": "Hello, world!",
    "stream": False
})

print(response.json())
```

### 2. éœ€æ±‚æå–æµ‹è¯•
```python
from app.requirements_parser.extractors.langchain_extractor import LangChainExtractor, AIProvider
from app.requirements_parser.models.document import Document, DocumentType

# åˆ›å»ºæå–å™¨
extractor = LangChainExtractor(provider=AIProvider.OLLAMA, model="llama2")

# æµ‹è¯•æ–‡æ¡£
document = Document(
    title="ç”¨æˆ·ç™»å½•éœ€æ±‚",
    content="ç”¨æˆ·éœ€è¦èƒ½å¤Ÿé€šè¿‡é‚®ç®±å’Œå¯†ç ç™»å½•ç³»ç»Ÿ",
    document_type=DocumentType.MARKDOWN
)

# æå–éœ€æ±‚
requirements = await extractor.extract_async(document)
print(f"æå–åˆ° {len(requirements)} ä¸ªéœ€æ±‚")
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ç«¯å£å ç”¨
```bash
# æ£€æŸ¥ç«¯å£
netstat -an | grep 11434

# æ›´æ”¹ç«¯å£
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

#### 2. å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull llama2:7b-q4_0  # é‡åŒ–ç‰ˆæœ¬ï¼Œæ›´çœå†…å­˜
```

#### 3. å“åº”æ…¢
```bash
# æ£€æŸ¥GPUæ”¯æŒ
ollama run llama2 --verbose

# ä½¿ç”¨CPUä¼˜åŒ–
OLLAMA_NUM_PARALLEL=4 ollama serve
```

#### 4. ä¸­æ–‡æ”¯æŒé—®é¢˜
```python
# ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„æç¤ºè¯
system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡éœ€æ±‚åˆ†æå¸ˆã€‚
è¯·ç”¨ä¸­æ–‡åˆ†ææ–‡æ¡£å¹¶æå–éœ€æ±‚ä¿¡æ¯ã€‚
è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONã€‚"""
```

## ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### Dockeréƒ¨ç½²
```dockerfile
# Dockerfile.ollama
FROM ollama/ollama:latest

# é¢„ä¸‹è½½æ¨¡å‹
RUN ollama pull llama2

EXPOSE 11434
CMD ["ollama", "serve"]
```

### docker-composeé›†æˆ
```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      
  testmind-ai:
    build: .
    environment:
      - AI_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama2
    depends_on:
      - ollama

volumes:
  ollama_data:
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ç¡¬ä»¶è¦æ±‚
- **æœ€ä½é…ç½®**: 8GB RAM, 4æ ¸CPU
- **æ¨èé…ç½®**: 16GB RAM, 8æ ¸CPU  
- **æœ€ä½³é…ç½®**: 32GB RAM, 16æ ¸CPU + GPU

### è½¯ä»¶ä¼˜åŒ–
```bash
# å¯ç”¨GPUåŠ é€Ÿ (NVIDIA)
OLLAMA_GPU=1 ollama serve

# è°ƒæ•´å¹¶å‘æ•°
OLLAMA_NUM_PARALLEL=2 ollama serve

# è®¾ç½®æ¨¡å‹ä¿æŒæ—¶é—´
OLLAMA_KEEP_ALIVE=10m ollama serve
```

## ğŸ‰ å¼€å§‹ä½¿ç”¨

1. **å®‰è£…Ollama**: é€‰æ‹©é€‚åˆä½ ç³»ç»Ÿçš„å®‰è£…æ–¹å¼
2. **ä¸‹è½½æ¨¡å‹**: æ¨èä»`llama2`å¼€å§‹
3. **å¯åŠ¨æœåŠ¡**: `ollama serve`
4. **é…ç½®TestMind AI**: è®¾ç½®ç¯å¢ƒå˜é‡
5. **å¼€å§‹æå–éœ€æ±‚**: äº«å—å…è´¹çš„AIéœ€æ±‚åˆ†æï¼

ç°åœ¨æ‚¨å¯ä»¥å®Œå…¨å…è´¹åœ°ä½¿ç”¨AIè¿›è¡Œéœ€æ±‚æå–ï¼Œæ— éœ€æ‹…å¿ƒAPIè´¹ç”¨ï¼ğŸŠ
