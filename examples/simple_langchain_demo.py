#!/usr/bin/env python3
"""
ç®€å•çš„LangChainéœ€æ±‚æå–å™¨æ¼”ç¤º
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ¸…ç†åçš„LangChainå¤šä¾›åº”å•†éœ€æ±‚æå–å™¨
"""
import asyncio
from app.requirements_parser.extractors.langchain_extractor import LangChainExtractor, AIProvider
from app.requirements_parser.models.document import Document, DocumentType


def create_sample_document() -> Document:
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    content = """# ç§»åŠ¨åº”ç”¨éœ€æ±‚æ–‡æ¡£

## åŠŸèƒ½éœ€æ±‚

### ç”¨æˆ·è®¤è¯
- ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½
- ç”¨æˆ·ç™»å½•åŠŸèƒ½
- å¿˜è®°å¯†ç åŠŸèƒ½

### å†…å®¹ç®¡ç†
- æ–‡ç« æµè§ˆåŠŸèƒ½
- æ–‡ç« æœç´¢åŠŸèƒ½
- æ”¶è—åŠŸèƒ½

## éåŠŸèƒ½æ€§éœ€æ±‚

### æ€§èƒ½è¦æ±‚
- åº”ç”¨å¯åŠ¨æ—¶é—´ä¸è¶…è¿‡3ç§’
- é¡µé¢åŠ è½½æ—¶é—´ä¸è¶…è¿‡2ç§’

### å…¼å®¹æ€§è¦æ±‚
- æ”¯æŒiOS 14+
- æ”¯æŒAndroid 8+
"""
    
    return Document(
        title="ç§»åŠ¨åº”ç”¨éœ€æ±‚æ–‡æ¡£",
        content=content,
        document_type=DocumentType.MARKDOWN
    )


async def demo_ollama_extraction():
    """æ¼”ç¤ºä½¿ç”¨Ollamaè¿›è¡Œéœ€æ±‚æå–"""
    print("ğŸš€ LangChainéœ€æ±‚æå–å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æ¡£
    document = create_sample_document()
    print(f"ğŸ“„ æ–‡æ¡£: {document.title}")
    print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(document.content)} å­—ç¬¦")
    
    # ä½¿ç”¨Ollamaæå–å™¨ï¼ˆä½¿ç”¨æ‚¨æœ¬åœ°çš„qwen3:4bæ¨¡å‹ï¼‰
    print(f"\nğŸ”§ åˆå§‹åŒ–LangChainæå–å™¨...")
    extractor = LangChainExtractor(
        provider=AIProvider.OLLAMA,
        model="qwen3:4b",
        temperature=0.1
    )
    
    print(f"âœ… æå–å™¨é…ç½®:")
    print(f"   ä¾›åº”å•†: {extractor.provider.value}")
    print(f"   æ¨¡å‹: {extractor.model}")
    print(f"   æ¸©åº¦: {extractor.temperature}")
    
    try:
        # æ‰§è¡Œéœ€æ±‚æå–
        print(f"\nğŸ”„ å¼€å§‹æå–éœ€æ±‚...")
        requirements = await extractor.extract_async(document)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… æå–å®Œæˆï¼")
        print(f"ğŸ“Š æå–åˆ° {len(requirements)} ä¸ªéœ€æ±‚")
        
        # æ˜¾ç¤ºéœ€æ±‚è¯¦æƒ…
        print(f"\nğŸ“‹ éœ€æ±‚åˆ—è¡¨:")
        for i, req in enumerate(requirements[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"\n{i}. {req.title}")
            print(f"   ID: {req.id}")
            print(f"   ç±»å‹: {req.type}")
            print(f"   ä¼˜å…ˆçº§: {req.priority}")
            print(f"   æè¿°: {req.description[:80]}...")
            print(f"   éªŒæ”¶æ ‡å‡†: {len(req.acceptance_criteria)} é¡¹")
        
        if len(requirements) > 5:
            print(f"\n... è¿˜æœ‰ {len(requirements) - 5} ä¸ªéœ€æ±‚")
        
        # ç»Ÿè®¡ä¿¡æ¯
        functional_count = sum(1 for req in requirements if req.type == "functional")
        non_functional_count = sum(1 for req in requirements if req.type == "non_functional")
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   åŠŸèƒ½æ€§éœ€æ±‚: {functional_count} ä¸ª")
        print(f"   éåŠŸèƒ½æ€§éœ€æ±‚: {non_functional_count} ä¸ª")
        print(f"   æ€»è®¡: {len(requirements)} ä¸ª")
        
        return requirements
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿:")
        print(f"   1. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ (http://localhost:11434)")
        print(f"   2. qwen3:4bæ¨¡å‹å·²å®‰è£…")
        print(f"   3. ç½‘ç»œè¿æ¥æ­£å¸¸")
        return []


def demo_provider_switching():
    """æ¼”ç¤ºä¾›åº”å•†åˆ‡æ¢åŠŸèƒ½"""
    print(f"\nğŸ”„ ä¾›åº”å•†åˆ‡æ¢æ¼”ç¤º")
    print("=" * 30)
    
    # æ¼”ç¤ºä¸åŒä¾›åº”å•†çš„åˆå§‹åŒ–
    providers_config = [
        {
            "name": "Ollama (æœ¬åœ°)",
            "provider": AIProvider.OLLAMA,
            "model": "qwen3:4b",
            "available": True
        },
        {
            "name": "OpenAI",
            "provider": AIProvider.OPENAI,
            "model": "gpt-3.5-turbo",
            "available": False  # éœ€è¦APIå¯†é’¥
        },
        {
            "name": "Google Gemini",
            "provider": AIProvider.GEMINI,
            "model": "gemini-1.5-pro",
            "available": False  # éœ€è¦APIå¯†é’¥
        }
    ]
    
    for config in providers_config:
        print(f"\nğŸ“± {config['name']}:")
        print(f"   ä¾›åº”å•†: {config['provider'].value}")
        print(f"   æ¨èæ¨¡å‹: {config['model']}")
        print(f"   çŠ¶æ€: {'âœ… å¯ç”¨' if config['available'] else 'âŒ éœ€è¦APIå¯†é’¥'}")
        
        if config['available']:
            try:
                extractor = LangChainExtractor(
                    provider=config['provider'],
                    model=config['model']
                )
                print(f"   åˆå§‹åŒ–: âœ… æˆåŠŸ")
            except Exception as e:
                print(f"   åˆå§‹åŒ–: âŒ å¤±è´¥ - {e}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ¼”ç¤ºéœ€æ±‚æå–
        requirements = await demo_ollama_extraction()
        
        # æ¼”ç¤ºä¾›åº”å•†åˆ‡æ¢
        demo_provider_switching()
        
        print(f"\nâœ¨ æ¼”ç¤ºå®Œæˆï¼")
        
        if requirements:
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
            print(f"   1. å°è¯•ä¸åŒçš„æ¨¡å‹å‚æ•°ï¼ˆæ¸©åº¦ã€æ¨¡å‹åç§°ï¼‰")
            print(f"   2. æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æ¡£")
            print(f"   3. é…ç½®OpenAIæˆ–Gemini APIå¯†é’¥ä½“éªŒäº‘ç«¯æ¨¡å‹")
            print(f"   4. ä½¿ç”¨æ‰¹é‡å¤„ç†åŠŸèƒ½å¤„ç†å¤šä¸ªæ–‡æ¡£")
        
    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())
