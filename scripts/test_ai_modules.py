#!/usr/bin/env python3
"""
AIæ¨¡å—ä¸“é¡¹æµ‹è¯•è„šæœ¬
ä¸“é—¨æµ‹è¯•LangChain AIæ¨¡å—çš„åŠŸèƒ½
"""
import sys
import time
import asyncio
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.requirements_parser.extractors.langchain_extractor import LangChainExtractor, AIProvider
from app.requirements_parser.models.document import Document, DocumentType
from app.requirements_parser.models.requirement import RequirementType, Priority

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)


class AIModuleTester:
    """AIæ¨¡å—æµ‹è¯•å™¨"""

    def __init__(self):
        self.project_root = project_root
        self.test_results = []
        self.reports_dir = self.project_root / "test_reports"
        self.reports_dir.mkdir(exist_ok=True)

    def run_ai_tests(self, test_level="all", save_logs=True):
        """è¿è¡ŒAIæ¨¡å—æµ‹è¯•"""
        logger.info("ğŸ¤– TestMind AI - AIæ¨¡å—ä¸“é¡¹æµ‹è¯•")
        logger.info("=" * 60)
        logger.info(f"æµ‹è¯•çº§åˆ«: {test_level}")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"å·¥ä½œç›®å½•: {self.project_root}")

        start_time = time.time()

        # æ‰§è¡Œä¸åŒçº§åˆ«çš„æµ‹è¯•
        if test_level == "basic":
            self._run_basic_tests()
        elif test_level == "providers":
            self._run_provider_tests()
        elif test_level == "advanced":
            self._run_advanced_tests()
        else:
            self._run_all_tests()

        duration = time.time() - start_time

        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        if save_logs:
            self._save_test_logs(test_level, duration)

        self._display_results(duration)

        return all(result["success"] for result in self.test_results)
    
    def _run_basic_tests(self):
        """è¿è¡ŒåŸºç¡€AIæµ‹è¯•"""
        logger.info("\nğŸ§ª åŸºç¡€AIåŠŸèƒ½æµ‹è¯•")
        logger.info("-" * 40)
        
        # æµ‹è¯•1: MOCKæä¾›å•†åˆå§‹åŒ–
        self._test_mock_provider_initialization()
        
        # æµ‹è¯•2: åŸºç¡€éœ€æ±‚æå–
        self._test_basic_requirement_extraction()
        
        # æµ‹è¯•3: åŒæ­¥å’Œå¼‚æ­¥æ–¹æ³•
        self._test_sync_async_methods()
    
    def _run_provider_tests(self):
        """è¿è¡ŒAIæä¾›å•†æµ‹è¯•"""
        logger.info("\nğŸ”Œ AIæä¾›å•†æµ‹è¯•")
        logger.info("-" * 40)
        
        # æµ‹è¯•ä¸åŒAIæä¾›å•†çš„åˆå§‹åŒ–
        self._test_all_providers_initialization()
        
        # æµ‹è¯•æä¾›å•†é…ç½®éªŒè¯
        self._test_provider_configuration()
    
    def _run_advanced_tests(self):
        """è¿è¡Œé«˜çº§AIæµ‹è¯•"""
        logger.info("\nğŸš€ é«˜çº§AIåŠŸèƒ½æµ‹è¯•")
        logger.info("-" * 40)
        
        # æµ‹è¯•å‡†ç¡®ç‡è®¡ç®—
        self._test_accuracy_calculation()
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        self._test_batch_processing()
        
        # æµ‹è¯•éœ€æ±‚é›†åˆåˆ›å»º
        self._test_requirement_collection()
    
    def _run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰AIæµ‹è¯•"""
        self._run_basic_tests()
        self._run_provider_tests()
        self._run_advanced_tests()
    
    def _test_mock_provider_initialization(self):
        """æµ‹è¯•MOCKæä¾›å•†åˆå§‹åŒ–"""
        test_name = "MOCKæä¾›å•†åˆå§‹åŒ–"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            assert extractor.provider == AIProvider.MOCK
            assert extractor.model == "mock-model"
            assert hasattr(extractor, 'extract')
            assert hasattr(extractor, 'extract_async')
            
            self._record_success(test_name, "MOCKæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
            logger.info("âœ… MOCKæä¾›å•†åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ MOCKæä¾›å•†åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_basic_requirement_extraction(self):
        """æµ‹è¯•åŸºç¡€éœ€æ±‚æå–"""
        test_name = "åŸºç¡€éœ€æ±‚æå–"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            document = Document(
                title="ç”¨æˆ·ç®¡ç†ç³»ç»Ÿéœ€æ±‚",
                content="""# ç”¨æˆ·ç®¡ç†ç³»ç»Ÿéœ€æ±‚
                
## åŠŸèƒ½éœ€æ±‚

### 1. ç”¨æˆ·æ³¨å†Œ
- ç”¨æˆ·å¯ä»¥é€šè¿‡é‚®ç®±æ³¨å†Œ
- å¯†ç å¿…é¡»åŒ…å«å¤§å°å†™å­—æ¯å’Œæ•°å­—

### 2. ç”¨æˆ·ç™»å½•
- æ”¯æŒé‚®ç®±ç™»å½•
- æ”¯æŒè®°ä½ç™»å½•çŠ¶æ€
""",
                document_type=DocumentType.MARKDOWN
            )
            
            # å¼‚æ­¥æå–æµ‹è¯•
            requirements = asyncio.run(extractor.extract_async(document))
            
            assert len(requirements) >= 1
            req = requirements[0]
            assert req.id == "REQ-001"
            assert req.title == "æ¨¡æ‹Ÿéœ€æ±‚"
            assert req.type == RequirementType.FUNCTIONAL
            assert req.priority == Priority.MEDIUM
            assert len(req.acceptance_criteria) >= 1
            assert req.source_document == document.title
            
            self._record_success(test_name, f"æˆåŠŸæå– {len(requirements)} ä¸ªéœ€æ±‚")
            logger.info(f"âœ… åŸºç¡€éœ€æ±‚æå–æµ‹è¯•é€šè¿‡ - æå–äº† {len(requirements)} ä¸ªéœ€æ±‚")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ åŸºç¡€éœ€æ±‚æå–æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_sync_async_methods(self):
        """æµ‹è¯•åŒæ­¥å’Œå¼‚æ­¥æ–¹æ³•"""
        test_name = "åŒæ­¥å¼‚æ­¥æ–¹æ³•"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            document = Document(
                title="ç®€å•éœ€æ±‚",
                content="ç”¨æˆ·éœ€è¦ç™»å½•åŠŸèƒ½",
                document_type=DocumentType.MARKDOWN
            )
            
            # æµ‹è¯•åŒæ­¥æ–¹æ³•
            sync_requirements = extractor.extract(document)
            
            # æµ‹è¯•å¼‚æ­¥æ–¹æ³•
            async_requirements = asyncio.run(extractor.extract_async(document))
            
            assert len(sync_requirements) >= 1
            assert len(async_requirements) >= 1
            assert sync_requirements[0].title == async_requirements[0].title
            
            self._record_success(test_name, "åŒæ­¥å’Œå¼‚æ­¥æ–¹æ³•éƒ½æ­£å¸¸å·¥ä½œ")
            logger.info("âœ… åŒæ­¥å¼‚æ­¥æ–¹æ³•æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ åŒæ­¥å¼‚æ­¥æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_all_providers_initialization(self):
        """æµ‹è¯•æ‰€æœ‰AIæä¾›å•†åˆå§‹åŒ–"""
        test_name = "AIæä¾›å•†åˆå§‹åŒ–"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        providers_tested = 0
        providers_passed = 0
        
        try:
            # æµ‹è¯•MOCKæä¾›å•†
            mock_extractor = LangChainExtractor(provider=AIProvider.MOCK)
            assert mock_extractor.provider == AIProvider.MOCK
            providers_tested += 1
            providers_passed += 1
            logger.info("  âœ… MOCKæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•Ollamaæä¾›å•†
            ollama_extractor = LangChainExtractor(
                provider=AIProvider.OLLAMA,
                model="llama2",
                ollama_url="http://localhost:11434"
            )
            assert ollama_extractor.provider == AIProvider.OLLAMA
            assert ollama_extractor.model == "llama2"
            providers_tested += 1
            providers_passed += 1
            logger.info("  âœ… Ollamaæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•OpenAIæä¾›å•†ï¼ˆæ— å¯†é’¥åº”è¯¥å¤±è´¥ï¼‰
            try:
                LangChainExtractor(provider=AIProvider.OPENAI)
                logger.warning("  âš ï¸  OpenAIæä¾›å•†æ— å¯†é’¥åˆå§‹åŒ–åº”è¯¥å¤±è´¥")
            except ValueError:
                providers_tested += 1
                providers_passed += 1
                logger.info("  âœ… OpenAIæä¾›å•†æ­£ç¡®éªŒè¯å¯†é’¥")
            
            # æµ‹è¯•OpenAIæä¾›å•†ï¼ˆæœ‰å¯†é’¥ï¼‰
            openai_extractor = LangChainExtractor(
                provider=AIProvider.OPENAI,
                api_key="sk-test1234567890abcdef1234567890abcdef12345678"
            )
            assert openai_extractor.provider == AIProvider.OPENAI
            providers_tested += 1
            providers_passed += 1
            logger.info("  âœ… OpenAIæä¾›å•†æœ‰å¯†é’¥åˆå§‹åŒ–æˆåŠŸ")
            
            self._record_success(test_name, f"{providers_passed}/{providers_tested} æä¾›å•†æµ‹è¯•é€šè¿‡")
            logger.info(f"âœ… AIæä¾›å•†åˆå§‹åŒ–æµ‹è¯•é€šè¿‡ - {providers_passed}/{providers_tested}")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ AIæä¾›å•†åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_provider_configuration(self):
        """æµ‹è¯•æä¾›å•†é…ç½®éªŒè¯"""
        test_name = "æä¾›å•†é…ç½®éªŒè¯"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            # æµ‹è¯•æ— æ•ˆæä¾›å•†é”™è¯¯
            extractor = LangChainExtractor(provider=AIProvider.MOCK)
            extractor.provider = "invalid_provider"
            
            document = Document(
                title="æµ‹è¯•",
                content="æµ‹è¯•å†…å®¹",
                document_type=DocumentType.MARKDOWN
            )
            
            try:
                asyncio.run(extractor.extract_async(document))
                self._record_failure(test_name, "æ— æ•ˆæä¾›å•†åº”è¯¥æŠ›å‡ºå¼‚å¸¸")
            except Exception:
                self._record_success(test_name, "æ­£ç¡®å¤„ç†æ— æ•ˆæä¾›å•†")
                logger.info("âœ… æä¾›å•†é…ç½®éªŒè¯æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ æä¾›å•†é…ç½®éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_accuracy_calculation(self):
        """æµ‹è¯•å‡†ç¡®ç‡è®¡ç®—"""
        test_name = "å‡†ç¡®ç‡è®¡ç®—"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            document = Document(
                title="æµ‹è¯•æ–‡æ¡£",
                content="æµ‹è¯•å†…å®¹",
                document_type=DocumentType.MARKDOWN
            )
            
            result = asyncio.run(extractor.extract_with_accuracy(document, expected_count=2))
            
            assert "requirements" in result
            assert "accuracy" in result
            assert "confidence" in result
            assert isinstance(result["accuracy"], float)
            assert 0.0 <= result["accuracy"] <= 1.0
            
            self._record_success(test_name, f"å‡†ç¡®ç‡: {result['accuracy']:.2%}")
            logger.info(f"âœ… å‡†ç¡®ç‡è®¡ç®—æµ‹è¯•é€šè¿‡ - å‡†ç¡®ç‡: {result['accuracy']:.2%}")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ å‡†ç¡®ç‡è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_batch_processing(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†"""
        test_name = "æ‰¹é‡å¤„ç†"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            documents = [
                Document(title="æ–‡æ¡£1", content="å†…å®¹1", document_type=DocumentType.MARKDOWN),
                Document(title="æ–‡æ¡£2", content="å†…å®¹2", document_type=DocumentType.MARKDOWN)
            ]
            
            results = asyncio.run(extractor.extract_batch(documents))
            
            assert len(results) == 2
            assert "æ–‡æ¡£1" in results
            assert "æ–‡æ¡£2" in results
            assert len(results["æ–‡æ¡£1"]) >= 1
            assert len(results["æ–‡æ¡£2"]) >= 1
            
            total_requirements = sum(len(reqs) for reqs in results.values())
            self._record_success(test_name, f"æ‰¹é‡å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæå– {total_requirements} ä¸ªéœ€æ±‚")
            logger.info(f"âœ… æ‰¹é‡å¤„ç†æµ‹è¯•é€šè¿‡ - å¤„ç†äº† {len(documents)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_requirement_collection(self):
        """æµ‹è¯•éœ€æ±‚é›†åˆåˆ›å»º"""
        test_name = "éœ€æ±‚é›†åˆåˆ›å»º"
        logger.info(f"ğŸ” æµ‹è¯•: {test_name}")
        
        try:
            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
            
            document = Document(
                title="æµ‹è¯•æ–‡æ¡£",
                content="æµ‹è¯•å†…å®¹",
                document_type=DocumentType.MARKDOWN
            )
            
            requirements = asyncio.run(extractor.extract_async(document))
            collection = extractor.create_requirement_collection(requirements)
            
            assert collection is not None
            assert len(collection.requirements) == len(requirements)
            
            self._record_success(test_name, f"åˆ›å»ºåŒ…å« {len(requirements)} ä¸ªéœ€æ±‚çš„é›†åˆ")
            logger.info(f"âœ… éœ€æ±‚é›†åˆåˆ›å»ºæµ‹è¯•é€šè¿‡ - é›†åˆåŒ…å« {len(requirements)} ä¸ªéœ€æ±‚")
            
        except Exception as e:
            self._record_failure(test_name, str(e))
            logger.error(f"âŒ éœ€æ±‚é›†åˆåˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
    
    def _record_success(self, test_name, details):
        """è®°å½•æˆåŠŸçš„æµ‹è¯•"""
        self.test_results.append({
            "name": test_name,
            "success": True,
            "details": details,
            "timestamp": datetime.now()
        })
    
    def _record_failure(self, test_name, error):
        """è®°å½•å¤±è´¥çš„æµ‹è¯•"""
        self.test_results.append({
            "name": test_name,
            "success": False,
            "error": error,
            "timestamp": datetime.now()
        })

    def _save_test_logs(self, test_level, duration):
        """ä¿å­˜æµ‹è¯•æ—¥å¿—"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = self.reports_dir / f"ai_module_test_{test_level}_{timestamp}.log"

            # æ”¶é›†æ‰€æœ‰æ—¥å¿—ä¿¡æ¯
            log_content = []
            log_content.append(f"ğŸ¤– TestMind AI - AIæ¨¡å—ä¸“é¡¹æµ‹è¯•æ—¥å¿—")
            log_content.append(f"=" * 60)
            log_content.append(f"æµ‹è¯•çº§åˆ«: {test_level}")
            log_content.append(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            log_content.append(f"å·¥ä½œç›®å½•: {self.project_root}")
            log_content.append(f"æ€»æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
            log_content.append("")

            # æ·»åŠ æµ‹è¯•ç»“æœè¯¦æƒ…
            log_content.append("ğŸ“‹ æµ‹è¯•ç»“æœè¯¦æƒ…:")
            log_content.append("-" * 40)

            for i, result in enumerate(self.test_results, 1):
                status = "âœ… é€šè¿‡" if result["success"] else "âŒ å¤±è´¥"
                log_content.append(f"{i}. {result['name']}: {status}")
                log_content.append(f"   æ—¶é—´: {result['timestamp'].strftime('%H:%M:%S')}")

                if result["success"]:
                    log_content.append(f"   è¯¦æƒ…: {result.get('details', 'æµ‹è¯•é€šè¿‡')}")
                else:
                    log_content.append(f"   é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                log_content.append("")

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            total_tests = len(self.test_results)
            passed_tests = sum(1 for r in self.test_results if r["success"])
            failed_tests = total_tests - passed_tests

            log_content.append("ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
            log_content.append("-" * 40)
            log_content.append(f"æ€»æµ‹è¯•æ•°: {total_tests}")
            log_content.append(f"é€šè¿‡: {passed_tests}")
            log_content.append(f"å¤±è´¥: {failed_tests}")
            log_content.append(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "æˆåŠŸç‡: 0%")

            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(log_content))

            logger.info(f"ğŸ’¾ AIæ¨¡å—æµ‹è¯•æ—¥å¿—å·²ä¿å­˜: {log_file}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜AIæ¨¡å—æµ‹è¯•æ—¥å¿—å¤±è´¥: {e}")
    
    def _display_results(self, duration):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š AIæ¨¡å—æµ‹è¯•ç»“æœ")
        logger.info("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r["success"])
        failed_tests = total_tests - passed_tests
        
        logger.info(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        logger.info(f"ğŸ“ˆ æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"âœ… é€šè¿‡: {passed_tests}")
        logger.info(f"âŒ å¤±è´¥: {failed_tests}")
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "ğŸ¯ æˆåŠŸç‡: 0%")
        
        if failed_tests > 0:
            logger.info("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for result in self.test_results:
                if not result["success"]:
                    logger.info(f"  - {result['name']}: {result['error']}")
        
        if passed_tests == total_tests:
            logger.info("\nğŸ‰ æ‰€æœ‰AIæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        else:
            logger.info(f"\nâš ï¸  {failed_tests} ä¸ªAIæ¨¡å—æµ‹è¯•å¤±è´¥")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="AIæ¨¡å—ä¸“é¡¹æµ‹è¯•")
    parser.add_argument(
        "--level",
        choices=["basic", "providers", "advanced", "all"],
        default="all",
        help="æµ‹è¯•çº§åˆ«"
    )
    parser.add_argument(
        "--no-logs",
        action="store_true",
        help="ä¸ä¿å­˜æ—¥å¿—æ–‡ä»¶"
    )
    parser.add_argument(
        "--view-logs",
        action="store_true",
        help="æµ‹è¯•åæŸ¥çœ‹æ—¥å¿—æ–‡ä»¶"
    )

    args = parser.parse_args()

    tester = AIModuleTester()
    success = tester.run_ai_tests(test_level=args.level, save_logs=not args.no_logs)

    # å¦‚æœéœ€è¦æŸ¥çœ‹æ—¥å¿—
    if args.view_logs and not args.no_logs:
        # æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = list(tester.reports_dir.glob(f"ai_module_test_{args.level}_*.log"))
        if log_files:
            latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
            print(f"\nğŸ“ æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {latest_log}")
            print("-" * 60)
            with open(latest_log, 'r', encoding='utf-8') as f:
                print(f.read())

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
