#!/usr/bin/env python3
"""
æ£€æŸ¥æœ¬åœ°Ollamaæ¨¡å‹
"""
import httpx
import json
import asyncio


async def check_ollama_models():
    """æ£€æŸ¥æœ¬åœ°Ollamaæ¨¡å‹"""
    ollama_url = "http://localhost:11434"
    
    try:
        async with httpx.AsyncClient() as client:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            print("ğŸ” æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€...")
            try:
                response = await client.get(f"{ollama_url}/api/version")
                if response.status_code == 200:
                    version_info = response.json()
                    print(f"âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
                    print(f"   ç‰ˆæœ¬: {version_info.get('version', 'unknown')}")
                else:
                    print(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                    return
            except Exception as e:
                print(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
                print(f"   è¯·ç¡®ä¿Ollamaåœ¨ {ollama_url} è¿è¡Œ")
                return
            
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            print(f"\nğŸ“‹ è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨...")
            try:
                response = await client.get(f"{ollama_url}/api/tags")
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get('models', [])
                    
                    if models:
                        print(f"âœ… æ‰¾åˆ° {len(models)} ä¸ªæœ¬åœ°æ¨¡å‹:")
                        print("-" * 50)
                        for i, model in enumerate(models, 1):
                            name = model.get('name', 'unknown')
                            size = model.get('size', 0)
                            modified = model.get('modified_at', 'unknown')
                            
                            # è½¬æ¢å¤§å°ä¸ºå¯è¯»æ ¼å¼
                            if size > 1024**3:
                                size_str = f"{size / (1024**3):.1f} GB"
                            elif size > 1024**2:
                                size_str = f"{size / (1024**2):.1f} MB"
                            else:
                                size_str = f"{size} bytes"
                            
                            print(f"{i}. {name}")
                            print(f"   å¤§å°: {size_str}")
                            print(f"   ä¿®æ”¹æ—¶é—´: {modified}")
                            print()
                        
                        # æ¨èæ¨¡å‹
                        print("ğŸ’¡ æ¨èä½¿ç”¨çš„æ¨¡å‹:")
                        for model in models:
                            name = model.get('name', '')
                            if 'qwen' in name.lower():
                                print(f"   - {name} (Qwenç³»åˆ—)")
                            elif 'llama' in name.lower():
                                print(f"   - {name} (Llamaç³»åˆ—)")
                            elif 'gemma' in name.lower():
                                print(f"   - {name} (Gemmaç³»åˆ—)")
                    else:
                        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°æ¨¡å‹")
                        print("   è¯·ä½¿ç”¨ 'ollama pull <model_name>' ä¸‹è½½æ¨¡å‹")
                else:
                    print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
            except Exception as e:
                print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {e}")
                
    except Exception as e:
        print(f"âŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


def update_extractor_config(model_name: str):
    """æ›´æ–°æå–å™¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°"""
    print(f"\nğŸ”§ å¦‚ä½•ä½¿ç”¨æ¨¡å‹ '{model_name}':")
    print("1. åœ¨ä»£ç ä¸­ä½¿ç”¨:")
    print(f"""
from app.requirements_parser.extractors.langchain_extractor_real import LangChainExtractorReal, AIProvider

extractor = LangChainExtractorReal(
    provider=AIProvider.OLLAMA,
    model="{model_name}",
    ollama_url="http://localhost:11434"
)
""")
    
    print("2. æˆ–è€…ä¿®æ”¹é»˜è®¤é…ç½®:")
    print(f"   åœ¨ langchain_extractor_real.py ä¸­å°†é»˜è®¤æ¨¡å‹æ”¹ä¸º: {model_name}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Ollamaæ¨¡å‹æ£€æŸ¥å·¥å…·")
    print("=" * 50)
    
    await check_ollama_models()
    
    print("\n" + "=" * 50)
    print("ğŸ“ ä½¿ç”¨è¯´æ˜:")
    print("1. å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ 'ollama pull <model_name>' ä¸‹è½½")
    print("2. æ¨èçš„è½»é‡çº§æ¨¡å‹:")
    print("   - ollama pull qwen2.5:4b")
    print("   - ollama pull llama3.2:3b")
    print("   - ollama pull gemma2:2b")
    print("3. æ‰¾åˆ°å¯ç”¨æ¨¡å‹åï¼Œæ›´æ–°ä»£ç ä¸­çš„æ¨¡å‹åç§°")


if __name__ == "__main__":
    asyncio.run(main())
