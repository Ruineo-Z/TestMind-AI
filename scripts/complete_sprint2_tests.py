#!/usr/bin/env python3
"""
Sprint 2 å®Œæ•´æµ‹è¯•å¥—ä»¶
åŒ…å«æ–‡æ¡£è§£æã€AIæ¨¡å—ã€APIæ¥å£çš„å…¨é¢æµ‹è¯•
"""
import sys
import subprocess
import time
import webbrowser
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class CompleteSprint2TestSuite:
    """Sprint 2 å®Œæ•´æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.project_root = project_root
        self.reports_dir = self.project_root / "test_reports"
        self.reports_dir.mkdir(exist_ok=True)
        self.test_results = {}
        
    def run_complete_tests(self, open_browser=True, save_logs=True):
        """è¿è¡Œå®Œæ•´çš„Sprint 2æµ‹è¯•"""
        print("ğŸ­ TestMind AI - Sprint 2 å®Œæ•´æµ‹è¯•å¥—ä»¶")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•èŒƒå›´: æ–‡æ¡£è§£æ + AIæ¨¡å— + APIæ¥å£ + é›†æˆæµ‹è¯•")
        print()

        start_time = time.time()

        # 1. æ–‡æ¡£è§£ææµ‹è¯•
        self._run_document_parsing_tests()

        # 2. AIæ¨¡å—æµ‹è¯•
        self._run_ai_module_tests()

        # 3. APIæ¥å£æµ‹è¯•
        self._run_api_tests()

        # 4. é›†æˆæµ‹è¯•
        self._run_integration_tests()

        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Šå’Œæ—¥å¿—
        duration = time.time() - start_time

        if save_logs:
            self._save_comprehensive_logs(duration)

        self._generate_comprehensive_report(duration, open_browser)

        return self._calculate_overall_success()
    
    def _run_document_parsing_tests(self):
        """è¿è¡Œæ–‡æ¡£è§£ææµ‹è¯•"""
        print("ğŸ“ 1. æ–‡æ¡£è§£ææ¨¡å—æµ‹è¯•")
        print("-" * 50)
        
        # è¿è¡Œæ–‡æ¡£è§£æå™¨å•å…ƒæµ‹è¯•
        test_files = [
            "tests/unit/test_markdown_parser.py",
            "tests/unit/test_pdf_parser.py", 
            "tests/unit/test_word_parser.py"
        ]
        
        for test_file in test_files:
            parser_name = test_file.split('_')[2].replace('.py', '').upper()
            print(f"  ğŸ” æµ‹è¯• {parser_name} è§£æå™¨...")
            
            result = self._run_pytest(test_file)
            self.test_results[f"document_parsing_{parser_name.lower()}"] = result
            
            if result["success"]:
                print(f"  âœ… {parser_name} è§£æå™¨æµ‹è¯•é€šè¿‡ ({result['duration']:.1f}s)")
            else:
                print(f"  âŒ {parser_name} è§£æå™¨æµ‹è¯•å¤±è´¥")
        
        print()
    
    def _run_ai_module_tests(self):
        """è¿è¡ŒAIæ¨¡å—æµ‹è¯•"""
        print("ğŸ¤– 2. AIæ¨¡å—æµ‹è¯•")
        print("-" * 50)

        # è¿è¡ŒLangChainæå–å™¨æµ‹è¯•
        print("  ğŸ” æµ‹è¯• LangChain éœ€æ±‚æå–å™¨...")
        result = self._run_pytest("tests/unit/test_langchain_extractor_simple.py")
        self.test_results["ai_langchain"] = result

        if result["success"]:
            print(f"  âœ… LangChain æå–å™¨æµ‹è¯•é€šè¿‡ ({result['duration']:.1f}s)")
        else:
            print(f"  âŒ LangChain æå–å™¨æµ‹è¯•å¤±è´¥")

        # è¿è¡ŒAIæ¨¡å—ä¸“é¡¹æµ‹è¯•
        print("  ğŸ” æµ‹è¯• AI æ¨¡å—ä¸“é¡¹åŠŸèƒ½...")
        ai_result = self._run_ai_module_script()
        self.test_results["ai_modules"] = ai_result

        if ai_result["success"]:
            print(f"  âœ… AI æ¨¡å—ä¸“é¡¹æµ‹è¯•é€šè¿‡ ({ai_result['duration']:.1f}s)")
        else:
            print(f"  âŒ AI æ¨¡å—ä¸“é¡¹æµ‹è¯•å¤±è´¥")

        # è¿è¡ŒAIæ¨¡å—è¯¦ç»†æµ‹è¯•æŠ¥å‘Š
        print("  ğŸ” ç”Ÿæˆ AI æ¨¡å—è¯¦ç»†æŠ¥å‘Š...")
        report_result = self._run_ai_module_report()
        self.test_results["ai_detailed_report"] = report_result

        if report_result["success"]:
            print(f"  âœ… AI æ¨¡å—è¯¦ç»†æŠ¥å‘Šç”ŸæˆæˆåŠŸ ({report_result['duration']:.1f}s)")
        else:
            print(f"  âŒ AI æ¨¡å—è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå¤±è´¥")

        print()
    
    def _run_api_tests(self):
        """è¿è¡ŒAPIæ¥å£æµ‹è¯•"""
        print("ğŸ”— 3. APIæ¥å£æµ‹è¯•")
        print("-" * 50)
        
        # è¿è¡ŒAPIå•å…ƒæµ‹è¯•
        print("  ğŸ” æµ‹è¯• API æ¥å£...")
        result = self._run_pytest("tests/unit/test_requirements_api.py")
        self.test_results["api_units"] = result
        
        if result["success"]:
            print(f"  âœ… API æ¥å£æµ‹è¯•é€šè¿‡ ({result['duration']:.1f}s)")
        else:
            print(f"  âŒ API æ¥å£æµ‹è¯•å¤±è´¥")
        
        print()
    
    def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("ğŸ”„ 4. é›†æˆæµ‹è¯•")
        print("-" * 50)
        
        # è¿è¡Œç”Ÿäº§çº§é›†æˆæµ‹è¯•
        print("  ğŸ” æµ‹è¯• ç”Ÿäº§çº§é›†æˆåŠŸèƒ½...")
        result = self._run_pytest("tests/integration/test_production_simple.py")
        self.test_results["integration"] = result
        
        if result["success"]:
            print(f"  âœ… é›†æˆæµ‹è¯•é€šè¿‡ ({result['duration']:.1f}s)")
        else:
            print(f"  âŒ é›†æˆæµ‹è¯•å¤±è´¥")
        
        print()
    
    def _run_pytest(self, test_file):
        """è¿è¡Œpytestæµ‹è¯•"""
        start_time = time.time()
        
        try:
            cmd = [
                sys.executable, "-m", "pytest",
                test_file,
                "-v", "--tb=short"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            duration = time.time() - start_time
            
            return {
                "success": result.returncode == 0,
                "duration": duration,
                "output": result.stdout,
                "error": result.stderr
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def _run_ai_module_script(self):
        """è¿è¡ŒAIæ¨¡å—ä¸“é¡¹æµ‹è¯•è„šæœ¬"""
        start_time = time.time()

        try:
            cmd = [
                sys.executable,
                str(self.project_root / "scripts" / "test_ai_modules_no_mock.py"),
                "--level", "all"
            ]

            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=60
            )

            duration = time.time() - start_time

            return {
                "success": result.returncode == 0,
                "duration": duration,
                "output": result.stdout,
                "error": result.stderr
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }

    def _run_ai_module_report(self):
        """è¿è¡ŒAIæ¨¡å—è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ"""
        start_time = time.time()

        try:
            cmd = [
                sys.executable,
                str(self.project_root / "scripts" / "ai_module_detailed_report.py"),
                "--no-browser"
            ]

            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=60
            )

            duration = time.time() - start_time

            return {
                "success": result.returncode == 0,
                "duration": duration,
                "output": result.stdout,
                "error": result.stderr
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def _generate_comprehensive_report(self, total_duration, open_browser):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“Š 5. ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("-" * 50)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.reports_dir / f"sprint2_complete_report_{timestamp}.html"
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r["success"])
        failed_tests = total_tests - passed_tests
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_content = self._create_html_report(total_duration, total_tests, passed_tests, failed_tests)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"  ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file.name}")
        print(f"  ğŸ“ æŠ¥å‘Šå¤§å°: {report_file.stat().st_size / 1024:.1f} KB")
        
        if open_browser:
            print("  ğŸŒ æ­£åœ¨æ‰“å¼€ç»¼åˆæŠ¥å‘Š...")
            webbrowser.open(f"file://{report_file.absolute()}")
        
        print()
    
    def _create_html_report(self, total_duration, total_tests, passed_tests, failed_tests):
        """åˆ›å»ºHTMLæŠ¥å‘Š"""
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # æµ‹è¯•æ¨¡å—è¯¦æƒ…
        modules_html = ""
        for module_name, result in self.test_results.items():
            status_icon = "âœ…" if result["success"] else "âŒ"
            status_class = "success" if result["success"] else "failure"
            
            modules_html += f"""
            <div class="module-card {status_class}">
                <h3>{status_icon} {module_name.replace('_', ' ').title()}</h3>
                <p>æ‰§è¡Œæ—¶é—´: {result['duration']:.2f}ç§’</p>
                <p>çŠ¶æ€: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}</p>
            </div>
            """
        
        return f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sprint 2 å®Œæ•´æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
        }}
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
        }}
        .stat-number {{
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }}
        .modules {{
            padding: 30px;
        }}
        .modules-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        .module-card {{
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
        }}
        .module-card.success {{
            background: #d4edda;
            border-left: 5px solid #28a745;
        }}
        .module-card.failure {{
            background: #f8d7da;
            border-left: 5px solid #dc3545;
        }}
        .footer {{
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
            color: #6c757d;
        }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
        .total {{ color: #007bff; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ­ Sprint 2 å®Œæ•´æµ‹è¯•æŠ¥å‘Š</h1>
            <p>TestMind AI - æ–‡æ¡£è§£æä¸AIéœ€æ±‚æå–ç³»ç»Ÿ</p>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number total">{total_tests}</div>
                <div>æµ‹è¯•æ¨¡å—</div>
            </div>
            <div class="stat-card">
                <div class="stat-number success">{passed_tests}</div>
                <div>é€šè¿‡æ¨¡å—</div>
            </div>
            <div class="stat-card">
                <div class="stat-number failure">{failed_tests}</div>
                <div>å¤±è´¥æ¨¡å—</div>
            </div>
            <div class="stat-card">
                <div class="stat-number total">{success_rate:.1f}%</div>
                <div>æˆåŠŸç‡</div>
            </div>
        </div>
        
        <div class="modules">
            <h2>ğŸ“‹ æµ‹è¯•æ¨¡å—è¯¦æƒ…</h2>
            <div class="modules-grid">
                {modules_html}
            </div>
        </div>
        
        <div class="footer">
            <p>æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’</p>
            <p>Â© 2025 TestMind AI - Sprint 2 å®Œæ•´æµ‹è¯•å¥—ä»¶</p>
        </div>
    </div>
</body>
</html>
"""
    
    def _save_comprehensive_logs(self, total_duration):
        """ä¿å­˜ç»¼åˆæµ‹è¯•æ—¥å¿—"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = self.reports_dir / f"sprint2_complete_test_{timestamp}.log"

            # æ”¶é›†æ‰€æœ‰æ—¥å¿—ä¿¡æ¯
            log_content = []
            log_content.append("ğŸ­ TestMind AI - Sprint 2 å®Œæ•´æµ‹è¯•æ—¥å¿—")
            log_content.append("=" * 70)
            log_content.append(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            log_content.append(f"æµ‹è¯•èŒƒå›´: æ–‡æ¡£è§£æ + AIæ¨¡å— + APIæ¥å£ + é›†æˆæµ‹è¯•")
            log_content.append(f"æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’")
            log_content.append("")

            # æ·»åŠ å„æ¨¡å—æµ‹è¯•ç»“æœ
            log_content.append("ğŸ“‹ æµ‹è¯•æ¨¡å—è¯¦æƒ…:")
            log_content.append("-" * 50)

            module_names = {
                "document_parsing_markdown": "Markdownè§£æå™¨",
                "document_parsing_pdf": "PDFè§£æå™¨",
                "document_parsing_word": "Wordè§£æå™¨",
                "ai_langchain": "LangChainæå–å™¨",
                "ai_modules": "AIæ¨¡å—ä¸“é¡¹",
                "api_units": "APIæ¥å£",
                "integration": "é›†æˆæµ‹è¯•"
            }

            for module_key, result in self.test_results.items():
                module_name = module_names.get(module_key, module_key)
                status = "âœ… é€šè¿‡" if result["success"] else "âŒ å¤±è´¥"
                log_content.append(f"{module_name}: {status} ({result['duration']:.2f}ç§’)")

                if not result["success"] and "error" in result:
                    log_content.append(f"  é”™è¯¯: {result['error']}")
                log_content.append("")

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            total_tests = len(self.test_results)
            passed_tests = sum(1 for r in self.test_results.values() if r["success"])
            failed_tests = total_tests - passed_tests

            log_content.append("ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
            log_content.append("-" * 50)
            log_content.append(f"æµ‹è¯•æ¨¡å—: {total_tests}")
            log_content.append(f"é€šè¿‡: {passed_tests}")
            log_content.append(f"å¤±è´¥: {failed_tests}")
            log_content.append(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "æˆåŠŸç‡: 0%")

            if failed_tests > 0:
                log_content.append("")
                log_content.append("âŒ å¤±è´¥çš„æ¨¡å—:")
                for module_key, result in self.test_results.items():
                    if not result["success"]:
                        module_name = module_names.get(module_key, module_key)
                        log_content.append(f"  - {module_name}")

            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(log_content))

            print(f"  ğŸ’¾ ç»¼åˆæµ‹è¯•æ—¥å¿—å·²ä¿å­˜: {log_file.name}")

        except Exception as e:
            print(f"  âŒ ä¿å­˜ç»¼åˆæµ‹è¯•æ—¥å¿—å¤±è´¥: {e}")

    def _calculate_overall_success(self):
        """è®¡ç®—æ€»ä½“æˆåŠŸç‡"""
        if not self.test_results:
            return False

        return all(result["success"] for result in self.test_results.values())
    
    def display_summary(self):
        """æ˜¾ç¤ºæµ‹è¯•æ€»ç»“"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r["success"])
        failed_tests = total_tests - passed_tests
        
        print("ğŸ“Š Sprint 2 æµ‹è¯•æ€»ç»“")
        print("=" * 70)
        print(f"ğŸ“ˆ æµ‹è¯•æ¨¡å—: {total_tests}")
        print(f"âœ… é€šè¿‡: {passed_tests}")
        print(f"âŒ å¤±è´¥: {failed_tests}")
        print(f"ğŸ¯ æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "ğŸ¯ æˆåŠŸç‡: 0%")
        
        if failed_tests > 0:
            print(f"\nâŒ å¤±è´¥çš„æ¨¡å—:")
            for module_name, result in self.test_results.items():
                if not result["success"]:
                    print(f"  - {module_name.replace('_', ' ').title()}")
        
        if passed_tests == total_tests:
            print(f"\nğŸ‰ Sprint 2 æ‰€æœ‰æµ‹è¯•æ¨¡å—é€šè¿‡ï¼")
            print(f"âœ¨ æ–‡æ¡£è§£æã€AIæ¨¡å—ã€APIæ¥å£ã€é›†æˆæµ‹è¯•å…¨éƒ¨æˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  {failed_tests} ä¸ªæ¨¡å—éœ€è¦ä¿®å¤")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Sprint 2 å®Œæ•´æµ‹è¯•å¥—ä»¶")
    parser.add_argument(
        "--no-browser",
        action="store_true",
        help="ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨"
    )
    parser.add_argument(
        "--no-logs",
        action="store_true",
        help="ä¸ä¿å­˜æ—¥å¿—æ–‡ä»¶"
    )
    parser.add_argument(
        "--view-logs",
        action="store_true",
        help="æµ‹è¯•åæŸ¥çœ‹æ—¥å¿—æ–‡ä»¶"
    )

    args = parser.parse_args()

    suite = CompleteSprint2TestSuite()
    success = suite.run_complete_tests(
        open_browser=not args.no_browser,
        save_logs=not args.no_logs
    )
    suite.display_summary()

    # å¦‚æœéœ€è¦æŸ¥çœ‹æ—¥å¿—
    if args.view_logs and not args.no_logs:
        log_files = list(suite.reports_dir.glob("sprint2_complete_test_*.log"))
        if log_files:
            latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
            print(f"\nğŸ“ æŸ¥çœ‹ç»¼åˆæµ‹è¯•æ—¥å¿—: {latest_log.name}")
            print("-" * 70)
            with open(latest_log, 'r', encoding='utf-8') as f:
                print(f.read())

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
