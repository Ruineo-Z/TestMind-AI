#!/usr/bin/env python3
"""
AIæ¨¡å—ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
æ·±å…¥æµ‹è¯•LangChain AIæ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½ï¼Œç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
"""
import sys
import time
import asyncio
import json
import webbrowser
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.requirements_parser.extractors.langchain_extractor import LangChainExtractor, AIProvider
from app.requirements_parser.models.document import Document, DocumentType
from app.requirements_parser.models.requirement import RequirementType, Priority


class AIModuleComprehensiveTest:
    """AIæ¨¡å—ç»¼åˆæµ‹è¯•å™¨"""

    def __init__(self):
        self.project_root = project_root
        self.reports_dir = self.project_root / "test_reports"
        self.reports_dir.mkdir(exist_ok=True)
        self.test_results = []
        self.performance_metrics = {}

    def run_comprehensive_test(self, open_browser=True):
        """è¿è¡Œç»¼åˆAIæ¨¡å—æµ‹è¯•"""
        print("ğŸ¤– TestMind AI - AIæ¨¡å—ç»¼åˆæµ‹è¯•")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•èŒƒå›´: LangChain + AIæä¾›å•† + éœ€æ±‚æå– + æ€§èƒ½åˆ†æ")
        print()

        start_time = time.time()

        # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
        self._test_basic_functionality()

        # 2. AIæä¾›å•†æµ‹è¯•
        self._test_ai_providers()

        # 3. éœ€æ±‚æå–è´¨é‡æµ‹è¯•
        self._test_extraction_quality()

        # 4. æ€§èƒ½æµ‹è¯•
        self._test_performance()

        # 5. è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        self._test_edge_cases()

        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        total_duration = time.time() - start_time
        self._generate_comprehensive_report(total_duration, open_browser)

        return self._calculate_success_rate()

    def _test_basic_functionality(self):
        """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
        print("ğŸ” 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        print("-" * 50)

        test_cases = [
            ("MOCKæå–å™¨åˆå§‹åŒ–", self._test_mock_initialization),
            ("åŸºç¡€éœ€æ±‚æå–", self._test_basic_extraction),
            ("åŒæ­¥å¼‚æ­¥ä¸€è‡´æ€§", self._test_sync_async_consistency),
            ("è‡ªå®šä¹‰æç¤ºè¯", self._test_custom_prompt),
        ]

        for test_name, test_func in test_cases:
            print(f"  ğŸ§ª {test_name}...")
            result = self._run_test(test_name, test_func)
            print(f"  {'âœ…' if result['success'] else 'âŒ'} {test_name}: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}")

        print()

    def _test_ai_providers(self):
        """æµ‹è¯•AIæä¾›å•†"""
        print("ğŸ”Œ 2. AIæä¾›å•†æµ‹è¯•")
        print("-" * 50)

        test_cases = [
            ("MOCKæä¾›å•†", self._test_mock_provider),
            ("OpenAIæä¾›å•†é…ç½®", self._test_openai_provider),
            ("Ollamaæä¾›å•†é…ç½®", self._test_ollama_provider),
            ("Geminiæä¾›å•†é…ç½®", self._test_gemini_provider),
            ("æ— æ•ˆæä¾›å•†å¤„ç†", self._test_invalid_provider),
        ]

        for test_name, test_func in test_cases:
            print(f"  ğŸ§ª {test_name}...")
            result = self._run_test(test_name, test_func)
            print(f"  {'âœ…' if result['success'] else 'âŒ'} {test_name}: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}")

        print()

    def _test_extraction_quality(self):
        """æµ‹è¯•éœ€æ±‚æå–è´¨é‡"""
        print("ğŸ“Š 3. éœ€æ±‚æå–è´¨é‡æµ‹è¯•")
        print("-" * 50)

        test_cases = [
            ("å‡†ç¡®ç‡è®¡ç®—", self._test_accuracy_calculation),
            ("è´¨é‡éªŒè¯", self._test_quality_validation),
            ("éœ€æ±‚é›†åˆåˆ›å»º", self._test_requirement_collection),
            ("æ‰¹é‡å¤„ç†", self._test_batch_processing),
        ]

        for test_name, test_func in test_cases:
            print(f"  ğŸ§ª {test_name}...")
            result = self._run_test(test_name, test_func)
            print(f"  {'âœ…' if result['success'] else 'âŒ'} {test_name}: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}")

        print()

    def _test_performance(self):
        """æµ‹è¯•æ€§èƒ½"""
        print("âš¡ 4. æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        test_cases = [
            ("å•æ–‡æ¡£å¤„ç†æ€§èƒ½", self._test_single_document_performance),
            ("æ‰¹é‡å¤„ç†æ€§èƒ½", self._test_batch_performance),
            ("å†…å­˜ä½¿ç”¨æµ‹è¯•", self._test_memory_usage),
            ("å¹¶å‘å¤„ç†æµ‹è¯•", self._test_concurrent_processing),
        ]

        for test_name, test_func in test_cases:
            print(f"  ğŸ§ª {test_name}...")
            result = self._run_test(test_name, test_func)
            print(f"  {'âœ…' if result['success'] else 'âŒ'} {test_name}: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}")

        print()

    def _test_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæ¡ä»¶"""
        print("ğŸ”¬ 5. è¾¹ç•Œæ¡ä»¶æµ‹è¯•")
        print("-" * 50)

        test_cases = [
            ("ç©ºæ–‡æ¡£å¤„ç†", self._test_empty_document),
            ("è¶…é•¿æ–‡æ¡£å¤„ç†", self._test_large_document),
            ("ç‰¹æ®Šå­—ç¬¦å¤„ç†", self._test_special_characters),
            ("é”™è¯¯æ¢å¤", self._test_error_recovery),
        ]

        for test_name, test_func in test_cases:
            print(f"  ğŸ§ª {test_name}...")
            result = self._run_test(test_name, test_func)
            print(f"  {'âœ…' if result['success'] else 'âŒ'} {test_name}: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}")

        print()

    def _run_test(self, test_name: str, test_func) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        start_time = time.time()

        try:
            result = test_func()
            duration = time.time() - start_time

            test_result = {
                "name": test_name,
                "success": True,
                "duration": duration,
                "details": result if isinstance(result, dict) else {"message": "æµ‹è¯•é€šè¿‡"},
                "timestamp": datetime.now()
            }

        except Exception as e:
            duration = time.time() - start_time
            test_result = {
                "name": test_name,
                "success": False,
                "duration": duration,
                "error": str(e),
                "timestamp": datetime.now()
            }

        self.test_results.append(test_result)
        return test_result

    # å…·ä½“æµ‹è¯•æ–¹æ³•å®ç°
    def _test_mock_initialization(self):
        """æµ‹è¯•MOCKæå–å™¨åˆå§‹åŒ–"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")
        assert extractor.provider == AIProvider.MOCK
        assert extractor.model == "mock-model"
        assert hasattr(extractor, 'extract')
        assert hasattr(extractor, 'extract_async')
        return {"provider": "MOCK", "model": "mock-model"}

    def _test_basic_extraction(self):
        """æµ‹è¯•åŸºç¡€éœ€æ±‚æå–"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="ç”¨æˆ·ç®¡ç†ç³»ç»Ÿéœ€æ±‚",
            content="""# ç”¨æˆ·ç®¡ç†ç³»ç»Ÿéœ€æ±‚

## åŠŸèƒ½éœ€æ±‚
1. ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½
2. ç”¨æˆ·ç™»å½•åŠŸèƒ½

## éåŠŸèƒ½éœ€æ±‚
- æ€§èƒ½è¦æ±‚ï¼šå“åº”æ—¶é—´ < 2ç§’
""",
            document_type=DocumentType.MARKDOWN
        )

        requirements = asyncio.run(extractor.extract_async(document))

        assert len(requirements) >= 1
        req = requirements[0]
        assert req.id == "REQ-001"
        assert req.title == "æ¨¡æ‹Ÿéœ€æ±‚"
        assert req.type == RequirementType.FUNCTIONAL
        assert req.priority == Priority.MEDIUM

        return {
            "requirements_count": len(requirements),
            "first_requirement_id": req.id,
            "extraction_method": "async"
        }

    def _test_sync_async_consistency(self):
        """æµ‹è¯•åŒæ­¥å¼‚æ­¥ä¸€è‡´æ€§"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="ç®€å•éœ€æ±‚",
            content="ç”¨æˆ·éœ€è¦ç™»å½•åŠŸèƒ½",
            document_type=DocumentType.MARKDOWN
        )

        # åŒæ­¥æ–¹æ³•
        sync_requirements = extractor.extract(document)

        # å¼‚æ­¥æ–¹æ³•
        async_requirements = asyncio.run(extractor.extract_async(document))

        assert len(sync_requirements) == len(async_requirements)
        assert sync_requirements[0].title == async_requirements[0].title

        return {
            "sync_count": len(sync_requirements),
            "async_count": len(async_requirements),
            "consistent": sync_requirements[0].title == async_requirements[0].title
        }

    def _test_custom_prompt(self):
        """æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="APIéœ€æ±‚",
            content="éœ€è¦è®¾è®¡ç”¨æˆ·API",
            document_type=DocumentType.MARKDOWN
        )

        custom_prompt = "è¯·ä¸“æ³¨äºAPIè®¾è®¡éœ€æ±‚çš„æå–"
        requirements = asyncio.run(extractor.extract_async(document, custom_prompt=custom_prompt))

        assert len(requirements) >= 1
        return {"custom_prompt_used": True, "requirements_count": len(requirements)}

    def _test_mock_provider(self):
        """æµ‹è¯•MOCKæä¾›å•†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK)
        assert extractor.provider == AIProvider.MOCK
        return {"provider": "MOCK", "initialization": "success"}

    def _test_openai_provider(self):
        """æµ‹è¯•OpenAIæä¾›å•†é…ç½®"""
        # æµ‹è¯•æ— å¯†é’¥åˆå§‹åŒ–ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
        try:
            LangChainExtractor(provider=AIProvider.OPENAI)
            return {"error": "åº”è¯¥æŠ›å‡ºå¯†é’¥é”™è¯¯"}
        except ValueError:
            pass  # é¢„æœŸçš„é”™è¯¯

        # æµ‹è¯•æœ‰å¯†é’¥åˆå§‹åŒ–
        extractor = LangChainExtractor(
            provider=AIProvider.OPENAI,
            api_key="sk-test1234567890abcdef1234567890abcdef12345678"
        )
        assert extractor.provider == AIProvider.OPENAI
        return {"provider": "OpenAI", "key_validation": "success"}

    def _test_ollama_provider(self):
        """æµ‹è¯•Ollamaæä¾›å•†é…ç½®"""
        extractor = LangChainExtractor(
            provider=AIProvider.OLLAMA,
            model="llama2",
            ollama_url="http://localhost:11434"
        )
        assert extractor.provider == AIProvider.OLLAMA
        assert extractor.model == "llama2"
        return {"provider": "Ollama", "model": "llama2", "url": "localhost:11434"}

    def _test_gemini_provider(self):
        """æµ‹è¯•Geminiæä¾›å•†é…ç½®"""
        extractor = LangChainExtractor(
            provider=AIProvider.GEMINI,
            api_key="test-gemini-key",
            model="gemini-pro"
        )
        assert extractor.provider == AIProvider.GEMINI
        return {"provider": "Gemini", "model": "gemini-pro"}

    def _test_invalid_provider(self):
        """æµ‹è¯•æ— æ•ˆæä¾›å•†å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK)
        extractor.provider = "invalid_provider"

        document = Document(
            title="æµ‹è¯•",
            content="æµ‹è¯•å†…å®¹",
            document_type=DocumentType.MARKDOWN
        )

        try:
            asyncio.run(extractor.extract_async(document))
            return {"error": "åº”è¯¥æŠ›å‡ºæ— æ•ˆæä¾›å•†é”™è¯¯"}
        except Exception:
            return {"invalid_provider_handled": True}

    def _test_accuracy_calculation(self):
        """æµ‹è¯•å‡†ç¡®ç‡è®¡ç®—"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="æµ‹è¯•æ–‡æ¡£",
            content="æµ‹è¯•å†…å®¹",
            document_type=DocumentType.MARKDOWN
        )

        result = asyncio.run(extractor.extract_with_accuracy(document, expected_count=2))

        assert "requirements" in result
        assert "accuracy" in result
        assert "confidence" in result
        assert isinstance(result["accuracy"], float)
        assert 0.0 <= result["accuracy"] <= 1.0

        return {
            "accuracy": result["accuracy"],
            "confidence": result["confidence"],
            "requirements_count": len(result["requirements"])
        }

    def _test_quality_validation(self):
        """æµ‹è¯•è´¨é‡éªŒè¯"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="è´¨é‡æµ‹è¯•",
            content="æµ‹è¯•è´¨é‡éªŒè¯åŠŸèƒ½",
            document_type=DocumentType.MARKDOWN
        )

        requirements = asyncio.run(extractor.extract_async(document))
        quality_result = extractor.validate_extraction_quality(requirements)

        assert "quality_score" in quality_result
        assert "issues" in quality_result
        assert "recommendations" in quality_result

        return {
            "quality_score": quality_result["quality_score"],
            "issues_count": len(quality_result["issues"]),
            "recommendations_count": len(quality_result["recommendations"])
        }

    def _test_requirement_collection(self):
        """æµ‹è¯•éœ€æ±‚é›†åˆåˆ›å»º"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="é›†åˆæµ‹è¯•",
            content="æµ‹è¯•éœ€æ±‚é›†åˆåˆ›å»º",
            document_type=DocumentType.MARKDOWN
        )

        requirements = asyncio.run(extractor.extract_async(document))
        collection = extractor.create_requirement_collection(requirements)

        assert collection.total_count == len(requirements)
        assert collection.requirements == requirements

        return {
            "total_count": collection.total_count,
            "functional_count": collection.functional_count,
            "collection_created": True
        }

    def _test_batch_processing(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        documents = [
            Document(title="æ–‡æ¡£1", content="å†…å®¹1", document_type=DocumentType.MARKDOWN),
            Document(title="æ–‡æ¡£2", content="å†…å®¹2", document_type=DocumentType.MARKDOWN)
        ]

        start_time = time.time()
        results = asyncio.run(extractor.extract_batch(documents))
        duration = time.time() - start_time

        assert len(results) == 2
        assert "æ–‡æ¡£1" in results
        assert "æ–‡æ¡£2" in results

        total_requirements = sum(len(reqs) for reqs in results.values())

        return {
            "documents_processed": len(documents),
            "total_requirements": total_requirements,
            "processing_time": duration,
            "avg_time_per_doc": duration / len(documents)
        }

    def _test_single_document_performance(self):
        """æµ‹è¯•å•æ–‡æ¡£å¤„ç†æ€§èƒ½"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # åˆ›å»ºä¸­ç­‰å¤§å°çš„æ–‡æ¡£
        content = "# éœ€æ±‚æ–‡æ¡£\n\n" + "\n\n".join([f"## éœ€æ±‚{i}\n\nè¿™æ˜¯éœ€æ±‚{i}çš„è¯¦ç»†æè¿°ã€‚" for i in range(1, 21)])

        document = Document(
            title="æ€§èƒ½æµ‹è¯•æ–‡æ¡£",
            content=content,
            document_type=DocumentType.MARKDOWN
        )

        # æµ‹é‡å¤„ç†æ—¶é—´
        start_time = time.time()
        requirements = asyncio.run(extractor.extract_async(document))
        duration = time.time() - start_time

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics["single_doc"] = {
            "document_size": len(content),
            "processing_time": duration,
            "requirements_count": len(requirements),
            "time_per_requirement": duration / max(len(requirements), 1)
        }

        return self.performance_metrics["single_doc"]

    def _test_batch_performance(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # åˆ›å»ºå¤šä¸ªæ–‡æ¡£
        documents = []
        for i in range(5):
            content = f"# æ–‡æ¡£{i}\n\n" + "\n\n".join([f"## éœ€æ±‚{j}\n\nè¿™æ˜¯éœ€æ±‚{j}çš„æè¿°ã€‚" for j in range(1, 6)])
            documents.append(Document(
                title=f"æ–‡æ¡£{i}",
                content=content,
                document_type=DocumentType.MARKDOWN
            ))

        # æµ‹é‡æ‰¹é‡å¤„ç†æ—¶é—´
        start_time = time.time()
        results = asyncio.run(extractor.extract_batch(documents))
        duration = time.time() - start_time

        total_requirements = sum(len(reqs) for reqs in results.values())

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics["batch"] = {
            "documents_count": len(documents),
            "total_processing_time": duration,
            "avg_time_per_doc": duration / len(documents),
            "total_requirements": total_requirements,
            "time_per_requirement": duration / max(total_requirements, 1)
        }

        return self.performance_metrics["batch"]

    def _test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        try:
            import psutil
            process = psutil.Process()

            # è®°å½•åˆå§‹å†…å­˜
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

            # åˆ›å»ºå¤§å‹æ–‡æ¡£
            content = "# å¤§å‹æ–‡æ¡£\n\n" + "\n\n".join([f"## ç« èŠ‚{i}\n\n" + "å†…å®¹ " * 100 for i in range(1, 11)])

            document = Document(
                title="å†…å­˜æµ‹è¯•æ–‡æ¡£",
                content=content,
                document_type=DocumentType.MARKDOWN
            )

            # å¤„ç†æ–‡æ¡£
            requirements = asyncio.run(extractor.extract_async(document))

            # è®°å½•æœ€ç»ˆå†…å­˜
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory

            # è®°å½•å†…å­˜æŒ‡æ ‡
            self.performance_metrics["memory"] = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "document_size_kb": len(content) / 1024,
                "requirements_count": len(requirements)
            }

            return self.performance_metrics["memory"]

        except ImportError:
            return {"error": "psutilæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•å†…å­˜ä½¿ç”¨"}

    def _test_concurrent_processing(self):
        """æµ‹è¯•å¹¶å‘å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # åˆ›å»ºå¤šä¸ªæ–‡æ¡£
        documents = []
        for i in range(3):
            content = f"# æ–‡æ¡£{i}\n\n" + "\n\n".join([f"## éœ€æ±‚{j}\n\nè¿™æ˜¯éœ€æ±‚{j}çš„æè¿°ã€‚" for j in range(1, 4)])
            documents.append(Document(
                title=f"æ–‡æ¡£{i}",
                content=content,
                document_type=DocumentType.MARKDOWN
            ))

        # å®šä¹‰å¼‚æ­¥å¤„ç†å‡½æ•°
        async def process_documents():
            start_time = time.time()

            # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡æ¡£
            tasks = [extractor.extract_async(doc) for doc in documents]
            results = await asyncio.gather(*tasks)

            duration = time.time() - start_time
            return results, duration

        # æ‰§è¡Œå¹¶å‘å¤„ç†
        results, duration = asyncio.run(process_documents())

        total_requirements = sum(len(reqs) for reqs in results)

        # è®°å½•å¹¶å‘æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics["concurrent"] = {
            "documents_count": len(documents),
            "total_processing_time": duration,
            "avg_time_per_doc": duration / len(documents),
            "total_requirements": total_requirements,
            "concurrent_speedup": True
        }

        return self.performance_metrics["concurrent"]

    def _test_empty_document(self):
        """æµ‹è¯•ç©ºæ–‡æ¡£å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        document = Document(
            title="ç©ºæ–‡æ¡£",
            content="",
            document_type=DocumentType.MARKDOWN
        )

        try:
            requirements = asyncio.run(extractor.extract_async(document))
            return {
                "handled_empty_document": True,
                "requirements_count": len(requirements)
            }
        except Exception as e:
            return {
                "handled_empty_document": False,
                "error": str(e)
            }

    def _test_large_document(self):
        """æµ‹è¯•è¶…é•¿æ–‡æ¡£å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # åˆ›å»ºè¶…é•¿æ–‡æ¡£
        content = "# è¶…é•¿æ–‡æ¡£\n\n" + "\n\n".join([f"## ç« èŠ‚{i}\n\n" + "å†…å®¹ " * 500 for i in range(1, 21)])

        document = Document(
            title="è¶…é•¿æ–‡æ¡£æµ‹è¯•",
            content=content,
            document_type=DocumentType.MARKDOWN
        )

        try:
            requirements = asyncio.run(extractor.extract_async(document))
            return {
                "handled_large_document": True,
                "document_size_kb": len(content) / 1024,
                "requirements_count": len(requirements)
            }
        except Exception as e:
            return {
                "handled_large_document": False,
                "document_size_kb": len(content) / 1024,
                "error": str(e)
            }

    def _test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # åˆ›å»ºåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æ¡£
        content = """# ç‰¹æ®Šå­—ç¬¦æµ‹è¯•

## éœ€æ±‚1
åŒ…å«ç‰¹æ®Šå­—ç¬¦: !@#$%^&*()_+{}|:"<>?~`-=[]\\;',./

## éœ€æ±‚2
åŒ…å«è¡¨æƒ…ç¬¦å·: ğŸ˜€ ğŸš€ ğŸ’¡ ğŸ”¥ ğŸ‘

## éœ€æ±‚3
åŒ…å«å¤šè¯­è¨€: English, ä¸­æ–‡, EspaÃ±ol, Ğ ÑƒÑÑĞºĞ¸Ğ¹, æ—¥æœ¬èª
        """

        document = Document(
            title="ç‰¹æ®Šå­—ç¬¦æµ‹è¯•",
            content=content,
            document_type=DocumentType.MARKDOWN
        )

        try:
            requirements = asyncio.run(extractor.extract_async(document))
            return {
                "handled_special_chars": True,
                "requirements_count": len(requirements)
            }
        except Exception as e:
            return {
                "handled_special_chars": False,
                "error": str(e)
            }

    def _test_error_recovery(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤"""
        extractor = LangChainExtractor(provider=AIProvider.MOCK, api_key="mock-key")

        # æ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ
        document = Document(
            title="é”™è¯¯æµ‹è¯•",
            content="æµ‹è¯•å†…å®¹",
            document_type=DocumentType.MARKDOWN
        )

        # å°è¯•ä½¿ç”¨æ— æ•ˆæä¾›å•†
        extractor.provider = "invalid_provider"

        try:
            # åº”è¯¥å¤±è´¥
            asyncio.run(extractor.extract_async(document))
            return {"error_recovery": False}
        except:
            # æ¢å¤åˆ°æœ‰æ•ˆæä¾›å•†
            extractor.provider = AIProvider.MOCK

            try:
                # åº”è¯¥æˆåŠŸ
                requirements = asyncio.run(extractor.extract_async(document))
                return {
                    "error_recovery": True,
                    "requirements_count": len(requirements)
                }
            except Exception as e:
                return {
                    "error_recovery": False,
                    "error": str(e)
                }
        }